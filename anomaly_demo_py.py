# -*- coding: utf-8 -*-
"""Anomaly_demo.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GdEsUrC_5PHy2Q86pLRbF7ih_7tUYXEe

# Python SIEM System Using AI and LLMs for Log Analysis and Anomaly Detection
"""

import pandas as pd

#sample log lines
logs = [
    "2025-03-06 08:00:00, INFO, User login success, user: admin",
    "2025-03-06 08:01:23, INFO, User login success, user: alice",
    "2025-03-06 08:02:45, ERROR, Failed login attempt, user: alice",
    # ... (more log lines)
]


'''
If the logs are in a text file, you can read them in Python.
For example, if each log entry is a line in the file,

with open("my_logs.txt") as f:
    raw_logs = f.readlines()
'''


'''
If the logs are in .csv file, Pandas can greatly simplify reading,

import pandas as pd
df = pd.read_csv("my_logs.csv")
print(df.head())
'''

parsed_logs = []
for line in logs:
    parts = [p.strip() for p in line.split(",")]
    timestamp = parts[0]
    level = parts[1]
    message = parts[2]
    user = parts[3].split(":")[1].strip() if "user:" in parts[3] else None
    parsed_logs.append({"timestamp": timestamp, "level": level, "message": message, "user": user})

# Convert to DataFrame for easier analysis
df_logs = pd.DataFrame(parsed_logs) # Now 'pd' is defined and can be used to create a DataFrame
print(df_logs.head())

"""## Preprocessing and Feature Extraction
generating a list of 50 values representing normal behavior, and then append a few values that are abnormally high:
"""

import numpy as np

# Simulate 50 minutes of normal login attempt counts (around 5 per minute on average)
np.random.seed(42)  # for reproducible example
normal_counts = np.random.poisson(lam=5, size=50)

# Simulate anomaly: a spike in login attempts (e.g., an attacker tries 30+ times in a minute)
anomalous_counts = np.array([30, 40, 50])

# Combine the data
login_attempts = np.concatenate([normal_counts, anomalous_counts])
print("Login attempts per minute:", login_attempts)

"""## Anomaly Detection Model

#### training the Isolation Forest on our login_attempts data.

###note:

####reshape login_attempts to a 2D array X with one feature column because scikit-learn requires a 2D array for training (fit).

####We set contamination=0.05 to give the model a hint that roughly 5% of the data might be anomalies. In our synthetic data we added 3 anomalies out of 53 points, which is ~5.7%, so 5% is a reasonable guess.
"""

from sklearn.ensemble import IsolationForest

# Prepare the data in the shape the model expects (samples, features)
X = login_attempts.reshape(-1, 1)  # each sample is a 1-dimensional [count]

# Initialize the Isolation Forest model
model = IsolationForest(contamination=0.05, random_state=42)
# contamination=0.05 means we expect about 5% of the data to be anomalies (or default is 0.1 in most cases)

# Train the model on the data
model.fit(X)

"""## Testing and Visualizing Results
using our trained model to detect anomalies in the log data. now the model predict labels for each data point and then filter out the ones flagged as outliers.
"""

# Use the model to predict anomalies
labels = model.predict(X)
# The model outputs +1 for normal points and -1 for anomalies

# Extract the anomaly indices and values
anomaly_indices = np.where(labels == -1)[0]
anomaly_values = login_attempts[anomaly_indices]

print("Anomaly indices:", anomaly_indices)
print("Anomaly values (login attempts):", anomaly_values)

import matplotlib.pyplot as plt

plt.plot(login_attempts, label="Login attempts per minute")
plt.scatter(anomaly_indices, anomaly_values, color='red', label="Anomalies")
plt.xlabel("Time (minute index)")
plt.ylabel("Login attempts")
plt.legend()
plt.show()

"""## Automated Response"""

if len(anomaly_indices) > 0:
    print(f"Alert! Detected {len(anomaly_indices)} anomalous events. Initiating response procedures...")
    # Here, you could add code to disable a user or notify an admin, etc.